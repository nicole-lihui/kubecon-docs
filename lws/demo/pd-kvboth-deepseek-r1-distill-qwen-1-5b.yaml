apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: pd-both-deepseek-r1-distill-qwen-1-5b
  namespace: demo-lws
spec:
  networkConfig:
    subdomainPolicy: UniquePerReplica
  replicas: 1
  rolloutStrategy:
    rollingUpdateConfiguration:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  startupPolicy: LeaderCreated
  leaderWorkerTemplate:
    size: 5
    subGroupPolicy:
      #subGroupPolicyType: LeaderWorker  # 或 LeaderExcluded
      subGroupSize: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          leaderworkerset.sigs.k8s.io/subdomainPolicy: UniquePerReplica
          role: leader
      spec:
        containers:
          - image: docker.1ms.run/lmcache/vllm-openai:2025-04-18
            command:
              - bash
              - -c
            args:
              - |
                python3 /scripts/disagg_proxy_server.py  \
                --host 0.0.0.0 --port 8000 \
                --prefiller-host pd-both-deepseek-r1-distill-qwen-1-5b-subgroup-0  --prefiller-port 8000 \
                --decoder-host pd-both-deepseek-r1-distill-qwen-1-5b-subgroup-1 --decoder-port 8000
            name: vllm-router
            ports:
              - containerPort: 8000
                protocol: TCP
            resources:
              requests:
                cpu: 100m
                memory: 200Mi
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              - mountPath: /data/serving-model
                name: model-path
              - mountPath: /scripts
                name: disagg-proxy-script
                readOnly: true
        nodeSelector:
          zestu.io/gpu.model: NVIDIA-GeForce-RTX-4090
        volumes:
          - emptyDir:
              medium: Memory
              sizeLimit: 10Gi
            name: dshm
          - name: model-path
            persistentVolumeClaim:
              claimName: deepseek-r1-distill-qwen-1-5b
          - name: disagg-proxy-script
            configMap:
              name: disagg-proxy-script
    workerTemplate:
      metadata:
        labels:
          role: worker
      spec:
        containers:
          - image: docker.1ms.run/lmcache/vllm-openai:2025-04-18
            args:
              - |
                vllm serve /data/serving-model --host 0.0.0.0 --port "8000" \
                --max-model-len 4096 \
                --served-model-name pd-both-deepseek-r1-distill-qwen-1-5b  \
                --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}'
            command:
              - /bin/bash
              - -c
            env:
              - name: LMCACHE_USE_EXPERIMENTAL
                value: "True"
              - name: LMCACHE_CHUNK_SIZE
                value: "256"
              - name: LMCACHE_LOCAL_CPU
                value: "False"
              - name: LMCACHE_MAX_LOCAL_CPU_SIZE
                value: "5.0"
              - name: LMCACHE_REMOTE_URL
                value: "lm://lmcache-server:8100"
              - name: LMCACHE_REMOTE_SERDE
                value: "naive"
            name: vllm-worker
            ports:
              - containerPort: 8000
                protocol: TCP
            resources:
              limits:
                nvidia.com/gpu: "1"
              requests:
                nvidia.com/gpu: "1"
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              - mountPath: /data/serving-model
                name: model-path
              - mountPath: /scripts
                name: disagg-proxy-script
                readOnly: true
        nodeSelector:
          zestu.io/gpu.model: NVIDIA-GeForce-RTX-4090
        volumes:
          - emptyDir:
              medium: Memory
              sizeLimit: 10Gi
            name: dshm
          - name: model-path
            persistentVolumeClaim:
              claimName: deepseek-r1-distill-qwen-1-5b
          - name: disagg-proxy-script
            configMap:
              name: disagg-proxy-script
---
apiVersion: v1
kind: Service
metadata:
  name: pd-both-deepseek-r1-distill-qwen-1-5b-subgroup-0
  namespace: demo-lws
  labels:
    prometheus-discovery: "true"
spec:
  selector:
    leaderworkerset.sigs.k8s.io/name: "pd-both-deepseek-r1-distill-qwen-1-5b"
    leaderworkerset.sigs.k8s.io/subgroup-index: "0" #see https://github.com/kubernetes-sigs/lws/issues/533
    role: worker
  type: ClusterIP
  ports:
    - name: vllm-service-port
      port: 8000
      protocol: TCP
      targetPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: pd-both-deepseek-r1-distill-qwen-1-5b-subgroup-1
  labels:
    prometheus-discovery: "true"
spec:
  selector:
    leaderworkerset.sigs.k8s.io/name: "pd-both-deepseek-r1-distill-qwen-1-5b"
    leaderworkerset.sigs.k8s.io/subgroup-index: "1"
    role: worker
  type: ClusterIP
  ports:
    - name: vllm-service-port
      port: 8000
      protocol: TCP
      targetPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: pd-both-deepseek-r1-distill-qwen-1-5b
spec:
  selector:
    leaderworkerset.sigs.k8s.io/name: pd-both-deepseek-r1-distill-qwen-1-5b # lws 模型的关联名称
    role: leader
  ports:
    - name: http
      protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP
